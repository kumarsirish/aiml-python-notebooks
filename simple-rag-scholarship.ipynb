{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4feb475",
   "metadata": {},
   "source": [
    "<h4>üì• Load scholarship data from Hugging Face dataset (parquet format)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3de218",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01642a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Read scholarship data from parquet file\n",
    "df = pd.read_parquet(\"hf://datasets/NetraVerse/indian-govt-scholarships/data/train-00000-of-00001.parquet\")\n",
    "\n",
    "# Select only text and label columns \n",
    "df = df[['label', 'text']]\n",
    "\n",
    "# Convert to records format\n",
    "data = df.to_dict('records')\n",
    "\n",
    "print(f\"Loaded {len(data)} scholarship documents\")\n",
    "pprint(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd4a13",
   "metadata": {},
   "source": [
    "<h4>‚úÖ Validate Dataset Quality and Structure</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHUNKING (Enable/Disable by setting flag)\n",
    "# ============================================\n",
    "# Set this to True to enable chunking, False to disable\n",
    "ENABLE_CHUNKING = True  # Change to False to disable chunking\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    '''Split text into overlapping chunks'''\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "if ENABLE_CHUNKING:\n",
    "    # Create chunked version of data\n",
    "    chunked_data = []\n",
    "    for doc in data:\n",
    "        text = doc['text']\n",
    "        chunks = chunk_text(text, chunk_size=500, overlap=100)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                'label': doc['label'],\n",
    "                'text': chunk,\n",
    "                'chunk_id': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            })\n",
    "    \n",
    "    # Replace original data with chunked data\n",
    "    data = chunked_data\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"‚úÖ CHUNKING ENABLED\")\n",
    "    print(f\"Chunked into {len(data)} pieces\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Display first chunk example - FULL TEXT\n",
    "    print(\"FIRST CHUNK EXAMPLE:\")\n",
    "    print(f\"Label: {data[0]['label']}\")\n",
    "    print(f\"Chunk ID: {data[0]['chunk_id']} of {data[0]['total_chunks']}\")\n",
    "    print(f\"Text Length: {len(data[0]['text'])} characters\")\n",
    "    print(f\"FULL TEXT:\\n{data[0]['text']}\")\n",
    "    print(f\"\\n{'='*50}\\n\")\n",
    "else:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"‚ùå CHUNKING DISABLED - Using full documents\")\n",
    "    print(f\"Total documents: {len(data)}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    print(\"FIRST DOCUMENT EXAMPLE:\")\n",
    "    print(f\"Label: {data[0]['label']}\")\n",
    "    print(f\"Text Length: {len(data[0]['text'])} characters\")\n",
    "    print(f\"FULL TEXT:\\n{data[0]['text']}\")\n",
    "    print(f\"\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169aa59e",
   "metadata": {},
   "source": [
    "<h4>üì¶ Install required dependencies for vector database, embeddings, and deep learning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install qdrant-client\n",
    "! pip install sentence-transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615e2c0",
   "metadata": {},
   "source": [
    "<h4>üîß Initialize Qdrant vector database client and SentenceTransformer embedding encoder</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# create the vector database client\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "\n",
    "\n",
    "# Create the embedding encoder\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fe9ce",
   "metadata": {},
   "source": [
    "<h4>üóÑÔ∏è Create vector collection for storing scholarship embeddings with cosine similarity</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store the scholarship data\n",
    "collection_name=\"scholarships\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a18dcf",
   "metadata": {},
   "source": [
    "<h4>‚¨ÜÔ∏è Generate embeddings for each document and upload to vector database</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e97d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_to_upload = []\n",
    "for idx, doc in enumerate(data):\n",
    "    points_to_upload.append(\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(doc[\"text\"]).tolist(),  # Use 'text' field for scholarship data\n",
    "            payload=doc\n",
    "        )\n",
    "    )\n",
    "\n",
    "# vectorize!\n",
    "qdrant.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=points_to_upload\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93270fa",
   "metadata": {},
   "source": [
    "<h4>‚¨ÜÔ∏è Check the embeddings</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first document's text and embedding\n",
    "first_doc = data[0]\n",
    "first_text = first_doc['text']\n",
    "first_vector = encoder.encode(first_text).tolist()\n",
    "\n",
    "print(\"DOCUMENT TEXT:\")\n",
    "print(f\"Text (first 500 chars): {first_text[:500]}...\")\n",
    "print(\"EMBEDDING VECTOR:\")\n",
    "print(f\"Vector dimension: {len(first_vector)}\")\n",
    "print(f\"First 20 values: {first_vector[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6d3e0",
   "metadata": {},
   "source": [
    "<h4>üí¨ Define user query for testing the RAG system</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c23040",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"what is the percetnage reservations for women in NSPG Scheme\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a91e9",
   "metadata": {},
   "source": [
    "<h4>üîç Convert user query into embedding vector for semantic search</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a640af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = encoder.encode(user_prompt).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4ab6b",
   "metadata": {},
   "source": [
    "<h4>üéØ Search vector database for top 3 most relevant scholarship documents</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74206db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search time for awesome wines!\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import SearchParams, ScoredPoint\n",
    "\n",
    "hits = qdrant.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589d8cd",
   "metadata": {},
   "source": [
    "<h4>üìÑ Display retrieved search results with metadata and similarity scores</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in hits.points: # Corrected: iterate over hits.points to get the ScoredPoint objects\n",
    "  pprint(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09621e9e",
   "metadata": {},
   "source": [
    "<h4>ü§ñ Load TinyLlama model and generate response WITHOUT retrieval context (baseline)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96276d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hugging Face models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Log in to Hugging Face Hub (requires a token set in Colab secrets as 'HF_TOKEN')\n",
    "# You can get a token from https://huggingface.co/settings/tokens and add it to Colab secrets.\n",
    "try:\n",
    "    hf_token =\"\"\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"[green]Successfully logged into Hugging Face Hub.\")\n",
    "    else:\n",
    "        print(\"Warning: Hugging Face token not found in Colab secrets. Some models might require authentication\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during Hugging Face login: {e}. Some models might not load.\")\n",
    "\n",
    "\n",
    "# Set up device (GPU if available, else CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot specializing in Indian government scholarships. Your top priority is to help users find relevant scholarship information and guide them with their queries. ONLY use information from the retrieved documents\"},\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tprompt,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048)\n",
    "pprint(\"Response without RAG and with TinyLlama:\")\n",
    "pprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b19537",
   "metadata": {},
   "source": [
    "<h4>üìã Extract payload data from search results for RAG augmentation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e88cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results\n",
    "search_results = [hit.payload for hit in hits.points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836ae62",
   "metadata": {},
   "source": [
    "<h4>‚ú® Generate response WITH retrieval context (RAG-enhanced)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the already-loaded model and tokenizer from the previous cell\n",
    "# No need to reload the model - just create a new prompt with RAG context\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a helpful chatbot specializing in Indian government scholarships. Use the following retrieved documents to answer the user's question accurately.ONLY use information from the retrieved documents.\\n\\nRetrieved Documents:\\n{str(search_results)}\"},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tprompt,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "pprint(\"Response with  RAG and with TinyLlama:\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "pprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEEP DIVE: Analyze Retrieved Chunks\n",
    "# ============================================\n",
    "\n",
    "print(\"üîç RETRIEVED CHUNKS ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"\\nüìÑ CHUNK {i}:\")\n",
    "    print(f\"Label: {result['label']}\")\n",
    "    print(f\"Chunk ID: {result.get('chunk_id', 'N/A')} of {result.get('total_chunks', 'N/A')}\")\n",
    "    print(f\"Text length: {len(result['text'])} characters\")\n",
    "    print(f\"\\nüìù FIRST 500 CHARACTERS OF TEXT:\")\n",
    "    print(result['text'][:500])\n",
    "    print(f\"\\nüìù LAST 200 CHARACTERS OF TEXT:\")\n",
    "    print(result['text'][-200:])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6120ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset using Hugging Face datasets library\n",
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"NetraVerse/indian-govt-scholarships\", split=\"train\")\n",
    "\n",
    "print(\"\\nüìä DATASET INFORMATION:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of rows: {len(dataset)}\")\n",
    "print(f\"Features/Columns: {dataset.features}\")\n",
    "print(f\"\\nüîç First record:\")\n",
    "print(\"-\" * 80)\n",
    "pprint(dataset[0])\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600df854",
   "metadata": {},
   "source": [
    "<h4>üîç Verify RAG Output - Check for Hallucinations</h4>\n",
    "<p>Compare the retrieved documents with the model's response to ensure accuracy</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8cc1b",
   "metadata": {},
   "source": [
    "<h4>üåê Launch interactive Gradio chatbot interface with full RAG pipeline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def scholarship_chatbot(message, history):\n",
    "    # Encode user query\n",
    "    query_vector = encoder.encode(message).tolist()\n",
    "    \n",
    "    # Search for relevant scholarships\n",
    "    hits = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    search_results = [hit.payload for hit in hits.points]\n",
    "    \n",
    "    # Generate response with LLM\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful chatbot specializing in Indian government scholarships. Use the following retrieved documents to answer accurately:\\n\\nRetrieved Documents:\\n{str(search_results)}\"},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=250)\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Launch Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    scholarship_chatbot,\n",
    "    title=\"üéì Indian Government Scholarship Chatbot\",\n",
    "    description=\"Ask me about Indian government scholarships!\",\n",
    "    examples=[\n",
    "        \"What scholarships are available for engineering students?\",\n",
    "        \"Tell me about AICTE scholarships\",\n",
    "        \"Are there scholarships for women in STEM?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
