{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4feb475",
   "metadata": {},
   "source": [
    "<h4>üì• Load scholarship data from Hugging Face dataset (parquet format)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01642a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Read scholarship data from parquet file\n",
    "df = pd.read_parquet(\"hf://datasets/NetraVerse/indian-govt-scholarships/data/train-00000-of-00001.parquet\")\n",
    "\n",
    "# Select only text and label columns (exclude binary file_name field)\n",
    "df = df[['label', 'text']]\n",
    "\n",
    "# Convert to records format\n",
    "data = df.to_dict('records')\n",
    "\n",
    "print(f\"Loaded {len(data)} scholarship documents\")\n",
    "pprint(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169aa59e",
   "metadata": {},
   "source": [
    "<h4>üì¶ Install required dependencies for vector database, embeddings, and deep learning</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install qdrant-client\n",
    "! pip install sentence-transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615e2c0",
   "metadata": {},
   "source": [
    "<h4>üîß Initialize Qdrant vector database client and SentenceTransformer embedding encoder</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# create the vector database client\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "\n",
    "\n",
    "# Create the embedding encoder\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fe9ce",
   "metadata": {},
   "source": [
    "<h4>üóÑÔ∏è Create vector collection for storing scholarship embeddings with cosine similarity</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store the scholarship data\n",
    "collection_name=\"scholarships\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a18dcf",
   "metadata": {},
   "source": [
    "<h4>‚¨ÜÔ∏è Generate embeddings for each document and upload to vector database</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e97d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_to_upload = []\n",
    "for idx, doc in enumerate(data):\n",
    "    points_to_upload.append(\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(doc[\"text\"]).tolist(),  # Use 'text' field for scholarship data\n",
    "            payload=doc\n",
    "        )\n",
    "    )\n",
    "\n",
    "# vectorize!\n",
    "qdrant.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=points_to_upload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7fdd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT TEXT:\n",
      "Text (first 500 chars): PRAGATI SCHOLARSHIP SCHEME...\n",
      "EMBEDDING VECTOR:\n",
      "Vector dimension: 384\n",
      "First 20 values: [-0.055145297199487686, 0.09013018757104874, -0.06578734517097473, -0.015903353691101074, -0.017773550003767014, -0.02971433289349079, -0.024064399302005768, 0.00018253223970532417, -0.0133947329595685, 0.017595848068594933, 0.06292257457971573, 0.0280141681432724, -0.04938691109418869, -0.004657465498894453, -0.00011058375093853101, -0.0664399191737175, -0.08881905674934387, 0.029758675023913383, 0.009087733924388885, -0.02971675992012024]\n"
     ]
    }
   ],
   "source": [
    "# Display first document's text and embedding\n",
    "first_doc = data[0]\n",
    "first_text = first_doc['text']\n",
    "first_vector = encoder.encode(first_text).tolist()\n",
    "\n",
    "print(\"DOCUMENT TEXT:\")\n",
    "print(f\"Text (first 500 chars): {first_text[:500]}...\")\n",
    "print(\"EMBEDDING VECTOR:\")\n",
    "print(f\"Vector dimension: {len(first_vector)}\")\n",
    "print(f\"First 20 values: {first_vector[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660c18b",
   "metadata": {},
   "source": [
    "<h4>üî¢ Example: View first document's text and its embedding vector</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6d3e0",
   "metadata": {},
   "source": [
    "<h4>üí¨ Define user query for testing the RAG system</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c23040",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Documents required for PRIME MINISTER‚ÄôS SCHOLARSHIP SCHEME for RPF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a91e9",
   "metadata": {},
   "source": [
    "<h4>üîç Convert user query into embedding vector for semantic search</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a640af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = encoder.encode(user_prompt).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4ab6b",
   "metadata": {},
   "source": [
    "<h4>üéØ Search vector database for top 3 most relevant scholarship documents</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74206db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search time for awesome wines!\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import SearchParams, ScoredPoint\n",
    "\n",
    "hits = qdrant.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589d8cd",
   "metadata": {},
   "source": [
    "<h4>üìÑ Display retrieved search results with metadata and similarity scores</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in hits.points: # Corrected: iterate over hits.points to get the ScoredPoint objects\n",
    "  pprint(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09621e9e",
   "metadata": {},
   "source": [
    "<h4>ü§ñ Load TinyLlama model and generate response WITHOUT retrieval context (baseline)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96276d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hugging Face models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Log in to Hugging Face Hub (requires a token set in Colab secrets as 'HF_TOKEN')\n",
    "# You can get a token from https://huggingface.co/settings/tokens and add it to Colab secrets.\n",
    "try:\n",
    "    hf_token =\"\"\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"[green]Successfully logged into Hugging Face Hub.\")\n",
    "    else:\n",
    "        print(\"Warning: Hugging Face token not found in Colab secrets. Some models might require authentication\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during Hugging Face login: {e}. Some models might not load.\")\n",
    "\n",
    "\n",
    "# Set up device (GPU if available, else CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot specializing in Indian government scholarships. Your top priority is to help users find relevant scholarship information and guide them with their queries.\"},\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tprompt,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048)\n",
    "pprint(\"Response without RAG and with TinyLlama:\")\n",
    "pprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b19537",
   "metadata": {},
   "source": [
    "<h4>üìã Extract payload data from search results for RAG augmentation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e88cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results\n",
    "search_results = [hit.payload for hit in hits.points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836ae62",
   "metadata": {},
   "source": [
    "<h4>‚ú® Generate response WITH retrieval context (RAG-enhanced)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hugging Face models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot specializing in Indian government scholarships. Your top priority is to help users find relevant scholarship information and guide them with their queries.\"},\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "    {\"role\": \"assistant\", \"content\": str(search_results)},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tprompt,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "pprint(\"Response with  RAG and with TinyLlama:\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=250)\n",
    "pprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8cc1b",
   "metadata": {},
   "source": [
    "<h4>üåê Launch interactive Gradio chatbot interface with full RAG pipeline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def scholarship_chatbot(message, history):\n",
    "    # Encode user query\n",
    "    query_vector = encoder.encode(message).tolist()\n",
    "    \n",
    "    # Search for relevant scholarships\n",
    "    hits = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    search_results = [hit.payload for hit in hits.points]\n",
    "    \n",
    "    # Generate response with LLM\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot specializing in Indian government scholarships. Help users find relevant scholarship information based on their queries.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Based on our database: {str(search_results)}\"}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=250)\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Launch Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    scholarship_chatbot,\n",
    "    title=\"üéì Indian Government Scholarship Chatbot\",\n",
    "    description=\"Ask me about Indian government scholarships!\",\n",
    "    examples=[\n",
    "        \"What scholarships are available for engineering students?\",\n",
    "        \"Tell me about AICTE scholarships\",\n",
    "        \"Are there scholarships for women in STEM?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
